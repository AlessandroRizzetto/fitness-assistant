{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score #accuracy score\n",
    "import pickle #library to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic # Holistic model that predicts 543 landmarks\n",
    "\n",
    "with open('fatigue.pkl', 'rb') as f:\n",
    "    model = pickle.load(f) #load the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for eyes detection\n",
    "eye_closed = False\n",
    "eye_closed_start_time = 0\n",
    "eye_closed_duration = 0\n",
    "blink_counter = 0\n",
    "blinks_rate_array = []\n",
    "blinks_times = []\n",
    "\n",
    "# Variables for average blinks per minute\n",
    "window_size = 5\n",
    "threshold_blinks = 5\n",
    "start_time = time.time()\n",
    "\n",
    "# Variables for mouth detection\n",
    "is_mouth_open = False\n",
    "lips_distance_threshold = 1\n",
    "\n",
    "# Variables for face color detection\n",
    "face_is_red = False\n",
    "base_red = 255\n",
    "base_green = 255\n",
    "base_blue = 255\n",
    "\n",
    "# Variables for score calculation\n",
    "#score = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_average(blinks_rate, window_size):\n",
    "    if len(blinks_rate) < window_size:\n",
    "        return None\n",
    "    window = blinks_rate[-window_size:] #get the last window_size blinks\n",
    "    average = sum(window) / window_size #calculate the average\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true if the person is fatigued\n",
    "def detect_anomalies(blinks_rate, window_size, threshold):\n",
    "    average = calculate_moving_average(blinks_rate, window_size)\n",
    "    if average is None:\n",
    "        return False\n",
    "    if average > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouth_is_open(results, threshold):\n",
    "    # extract the mouth landmarks\n",
    "    landmark_upperlip_center = results.face_landmarks.landmark[13]\n",
    "    landmark_bottomlip_center = results.face_landmarks.landmark[14]\n",
    "    #define the distance between the upper and bottom lip when the mouth is closed taking the minimum value\n",
    "    distance_Y = landmark_bottomlip_center.y - landmark_upperlip_center.y\n",
    "    if distance_Y < threshold and distance_Y > 0: # the distance has to be positive and less than the threshold\n",
    "        threshold = distance_Y #update the threshold with the new minimum value of distance_Y when the mouth is closed \n",
    "    if distance_Y > (threshold*10):\n",
    "        is_open = True\n",
    "    else:\n",
    "        is_open = False\n",
    "    \n",
    "    return is_open, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_face_red (frame, results, base_red, base_green, base_blue):\n",
    "    # select the landmarks of the face\n",
    "    face_landmarks = results.face_landmarks.landmark\n",
    "    x_coords = [int(landmark.x * frame.shape[1]) for landmark in face_landmarks]\n",
    "    y_coords = [int(landmark.y * frame.shape[0]) for landmark in face_landmarks]\n",
    "    mask = np.zeros(frame.shape[:2], dtype=np.uint8) #create a mask with the same shape of the frame\n",
    "    cv2.fillPoly(mask, [np.array(list(zip(x_coords, y_coords)))], (255, 255, 255)) #fill the mask with the face landmarks coordinates\n",
    "    mean_color = cv2.mean(frame, mask=mask) #calculate the mean color of the face in the frame using the mask created before\n",
    "    \n",
    "    blue, green, red, _ = mean_color\n",
    "    #base_blue, base_green, base_red, _ = base_color\n",
    "    #print(\"Face Color RGB: \", red, green, blue)\n",
    "    \n",
    "    #check if the green and blue values of the face are unless 40 points lower than the base color (the face is red)\n",
    "    if (base_green - green) > 40 and (base_blue - blue) > 40:\n",
    "        return True, red, green, blue\n",
    "    \n",
    "    #check if the mean color of the face is red\n",
    "    if red > 150 and green < 100 and blue < 100:\n",
    "        return True, red, green, blue \n",
    "    \n",
    "    else:\n",
    "        return False, red, green, blue \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score (model_class, is_mouth_open, face_is_red, blinks_rate_array, window_size, threshold_blinks, blinks_times):\n",
    "    score = 0\n",
    "    \n",
    "    if model_class == 'fatigue': \n",
    "        score = score + 40\n",
    "    if is_mouth_open:\n",
    "        score = score + 20\n",
    "    if face_is_red:\n",
    "        score = score + 20\n",
    "    if detect_anomalies(blinks_rate_array, window_size, threshold_blinks) | (blinks_times[-1] > 5):\n",
    "        score = score + 20\n",
    "    # print(score)\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "20\n",
      "20\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "20\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "20\n",
      "20\n",
      "0\n",
      "0\n",
      "20\n",
      "0\n",
      "0\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "40\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "40\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "40\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "0\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "20\n",
      "20\n",
      "20\n",
      "0\n",
      "0\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "60\n",
      "80\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "80\n",
      "80\n",
      "60\n",
      "60\n",
      "60\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark       \n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())         \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())        \n",
    "            elapsed_time = time.time() - start_time\n",
    "            # Concate rows\n",
    "            row = pose_row+face_row\n",
    "            \n",
    "            # Make Detections\n",
    "            x = pd.DataFrame([row])\n",
    "            model_class = model.predict(x)[0] #class of the model\n",
    "            model_prob = model.predict_proba(x)[0] #probability of each class\n",
    "            #print(model_class, model_prob)\n",
    "            \n",
    "            # Get status box\n",
    "            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Display Class\n",
    "            cv2.putText(image, 'CLASS'\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, model_class.split(' ')[0]\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display Probability\n",
    "            cv2.putText(image, 'PROB'\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(model_prob[np.argmax(model_prob)],2))\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            #Detect eye landmarks\n",
    "            if results.face_landmarks is not None:\n",
    "                left_eye_landmarks = [results.face_landmarks.landmark[33], results.face_landmarks.landmark[133], results.face_landmarks.landmark[159], results.face_landmarks.landmark[145], results.face_landmarks.landmark[153], results.face_landmarks.landmark[157]]\n",
    "                right_eye_landmarks = [results.face_landmarks.landmark[362], results.face_landmarks.landmark[263], results.face_landmarks.landmark[386], results.face_landmarks.landmark[374], results.face_landmarks.landmark[380], results.face_landmarks.landmark[382]]\n",
    "                # Calculate vertical distance between top and bottom of left eye\n",
    "                left_eye_top = left_eye_landmarks[0].y\n",
    "                left_eye_bottom = left_eye_landmarks[3].y\n",
    "                left_eye_vertical_distance = left_eye_bottom - left_eye_top\n",
    "                right_eye_top = right_eye_landmarks[0].y\n",
    "                right_eye_bottom = right_eye_landmarks[3].y\n",
    "                right_eye_vertical_distance = right_eye_bottom - right_eye_top\n",
    "                # Calculate horizontal distance between left and right of left eye\n",
    "                left_eye_left = left_eye_landmarks[4].x\n",
    "                left_eye_right = left_eye_landmarks[5].x\n",
    "                left_eye_horizontal_distance = left_eye_right - left_eye_left\n",
    "                right_eye_left = right_eye_landmarks[4].x\n",
    "                right_eye_right = right_eye_landmarks[5].x\n",
    "                right_eye_horizontal_distance = right_eye_right - right_eye_left\n",
    "                # Calculate eye aspect ratio\n",
    "                left_eye_aspect_ratio = left_eye_vertical_distance / left_eye_horizontal_distance\n",
    "                right_eye_aspect_ratio = right_eye_vertical_distance / right_eye_horizontal_distance\n",
    "                # Calculate average eye aspect ratio\n",
    "                average_eye_aspect_ratio = (left_eye_aspect_ratio + right_eye_aspect_ratio) / 2\n",
    "                # Detect if eyes are closed\n",
    "                if average_eye_aspect_ratio < 0.2:\n",
    "                    if not eye_closed: #if eyes were open before, start timer\n",
    "                        eye_closed_start_time = time.time()\n",
    "                        blink_counter += 1\n",
    "                        #print(blinks)\n",
    "                        #print(blinks_times)\n",
    "                    eye_closed = True\n",
    "                    cv2.putText(image, 'Eyes Closed', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                else:\n",
    "                    if eye_closed:\n",
    "                        eye_closed_duration = time.time() - eye_closed_start_time\n",
    "                        blinks_times.append(eye_closed_duration)\n",
    "                        #print(blinks_times[-1])\n",
    "                        #print(calculate_moving_average(blinks_rate_array, window_size)) # print moving average of blinks rate\n",
    "                        #print('Eyes were closed for ' + str(round(eye_closed_duration, 2)) + ' seconds')\n",
    "                    cv2.putText(image, 'Eyes Open', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                    eye_closed = False           \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # every 5 seconds, calculate blinks rate\n",
    "                if elapsed_time >= 5:\n",
    "                    blinking_rate = blink_counter / elapsed_time * 60\n",
    "                    blinks_rate_array.append(blinking_rate)\n",
    "                    #print('Blinking rate: ' + str(round(blinking_rate, 2)) + ' blinks per minute')\n",
    "                    #print('Blinks in 10 seconds: ' + str(blink_counter))\n",
    "                    blink_counter = 0\n",
    "                    start_time = time.time()\n",
    "                \n",
    "                # Removing old rates from array if more than 20 seconds have passed\n",
    "                if len(blinks_rate_array) > 20:\n",
    "                    blinks_rate_array.pop(0)\n",
    "                    blinks_times.pop(0)\n",
    "                \n",
    "                # Detect anomalies in blinks rate or if eyes are closed for more than 5 seconds\n",
    "                # if detect_anomalies(blinks_rate_array, window_size, threshold_blinks) | (blinks_times[-1] > 5):\n",
    "                #     cv2.putText(image, 'ANOMALY DETECTED', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    #print('ANOMALY DETECTED')\n",
    "\n",
    "                is_mouth_open, lips_distance_threshold = mouth_is_open(results, lips_distance_threshold)\n",
    "                #print(is_mouth_open, lips_distance_threshold)\n",
    "                \n",
    "                #extract the BGR values\n",
    "                face_is_red, red, green, blue = is_face_red(frame, results, base_red, base_green, base_blue)\n",
    "                if (base_red == 255) & (base_green == 255) & (base_blue == 255): #assign base color if it is not assigned yet\n",
    "                    base_red = red\n",
    "                    base_green = green\n",
    "                    base_blue = blue\n",
    "                    #print('Base color is assigned', base_red, base_green, base_blue)\n",
    "            \n",
    "                #print(face_is_red, red, green, blue)\n",
    "                \n",
    "                # Compute score\n",
    "                score = compute_score(model_class, is_mouth_open, face_is_red, blinks_rate_array, window_size, threshold_blinks, blinks_times)\n",
    "                \n",
    "                if score > 50 :\n",
    "                    print('Alert!')\n",
    "                \n",
    "    \n",
    "        except:\n",
    "            pass             \n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
